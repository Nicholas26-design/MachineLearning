{"cells":[{"cell_type":"markdown","source":["# Develop, evaluate, and score a forecasting model for electricity generation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bbdd4074-f4db-40f5-86d2-cbf2c3766f85"},{"cell_type":"code","source":["\"\"\"\n"," Introduction\n","\n","In this notebook, you'll see Microsoft Fabric's end-to-end data science workflow for a forecasting model. This scenario uses the historic sales data to predict the sales for different categories of products at a superstore.\n","\n","Forecasting is a crucial asset in sales, harnessing historical data and predictive methods to provide insights into future trends. By analyzing past sales, identifying patterns, and learning from consumer behavior, businesses can optimize inventory, production, and marketing strategies. This proactive approach enhances adaptability, responsiveness, and overall performance of businesses in a dynamic marketplace.\n","\n","The main steps in this notebook are:\n","\n","1. Load the data\n","2. Understand and process the data using exploratory data analysis\n","3. Train a machine learning model using an open source software package called `SARIMAX` and track experiments using MLflow and Fabric Autologging feature\n","4. Save the final machine learning model and make predictions\n","5. Demonstrate the model performance via visualizations in Power BI\n","\n","Thesis\n","\n","What percentage of fuel generation will come from Natural Gas in the coming months?\n","\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"b44d5d99-160a-4c42-b06b-f563272dffb5","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-01T19:25:49.0171206Z","session_start_time":"2024-01-01T19:25:49.4035898Z","execution_start_time":"2024-01-01T19:25:58.2243517Z","execution_finish_time":"2024-01-01T19:26:00.2423835Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e31168bf-b4b8-42e5-8bc6-44145c979cb0"},"text/plain":"StatementMeta(, b44d5d99-160a-4c42-b06b-f563272dffb5, 3, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"\"\\n Introduction\\n\\nIn this notebook, you'll see Microsoft Fabric's end-to-end data science workflow for a forecasting model. This scenario uses the historic sales data to predict the sales for different categories of products at a superstore.\\n\\nForecasting is a crucial asset in sales, harnessing historical data and predictive methods to provide insights into future trends. By analyzing past sales, identifying patterns, and learning from consumer behavior, businesses can optimize inventory, production, and marketing strategies. This proactive approach enhances adaptability, responsiveness, and overall performance of businesses in a dynamic marketplace.\\n\\nThe main steps in this notebook are:\\n\\n1. Load the data\\n2. Understand and process the data using exploratory data analysis\\n3. Train a machine learning model using an open source software package called `SARIMAX` and track experiments using MLflow and Fabric Autologging feature\\n4. Save the final machine learning model and make predictions\\n5. Demonstrate the model performance via visualizations in Power BI\\n\\nThesis\\n\\nWhat percentage of fuel generation will come from Natural Gas in the coming months?\\n\\n\""},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"24139096-b912-4e14-aee7-37615c14b18d"},{"cell_type":"code","source":["pip install pmdarima"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"b44d5d99-160a-4c42-b06b-f563272dffb5","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-01T19:25:49.0239587Z","session_start_time":null,"execution_start_time":"2024-01-01T19:26:00.7191498Z","execution_finish_time":"2024-01-01T19:26:10.8947814Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"72f6613b-742b-452f-86e6-e7198a3ed58e"},"text/plain":"StatementMeta(, b44d5d99-160a-4c42-b06b-f563272dffb5, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting pmdarima\n  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (1.3.2)\nRequirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (3.0.4)\nRequirement already satisfied: numpy>=1.21.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (1.24.3)\nRequirement already satisfied: pandas>=0.19 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (2.0.3)\nRequirement already satisfied: scikit-learn>=0.22 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (1.10.1)\nRequirement already satisfied: statsmodels>=0.13.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (0.14.0)\nRequirement already satisfied: urllib3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (1.26.17)\nRequirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (68.2.2)\nRequirement already satisfied: packaging>=17.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pmdarima) (23.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2023.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from scikit-learn>=0.22->pmdarima) (3.2.0)\nRequirement already satisfied: patsy>=0.5.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\nRequirement already satisfied: six in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\nInstalling collected packages: pmdarima\nSuccessfully installed pmdarima-2.0.4\nNote: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9d97e2fa-1402-49fe-a6d3-e1cb94282e68"},{"cell_type":"markdown","source":["## Libraries and Variables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e7d8d1c8-8e8b-446d-8e78-5d61164e2782"},{"cell_type":"code","source":["# Importing required libraries\n","import warnings\n","import itertools\n","# Record the notebook running time\n","import time\n","from datetime import timedelta\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# Set up MLflow for experiment tracking\n","import mlflow\n","warnings.filterwarnings(\"ignore\")\n","plt.style.use('fivethirtyeight')\n","import pandas as pd\n","import pmdarima as pm\n","import statsmodels.api as sm\n","import matplotlib\n","matplotlib.rcParams['axes.labelsize'] = 14\n","matplotlib.rcParams['xtick.labelsize'] = 12\n","matplotlib.rcParams['ytick.labelsize'] = 12\n","matplotlib.rcParams['text.color'] = 'k'\n","# Import required libraries for model evaluation\n","from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n","from statsmodels.tsa.stattools import adfuller, acf, pacf\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from pyspark.sql.functions import col, to_date\n","from pmdarima.arima import auto_arima\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import regexp_extract\n","from statsmodels.stats.diagnostic import acorr_ljungbox\n","import os, requests\n","import statsmodels.api as sm\n","from mlflow import MlflowClient\n","from pprint import pprint\n","\"\"\"\n","`Statsmodels` is a Python module that provides classes and functions for the estimation of many different statistical models\n","It also allows you to conduct statistical tests and statistical data exploration.\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"b44d5d99-160a-4c42-b06b-f563272dffb5","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-01T19:25:49.0245804Z","session_start_time":null,"execution_start_time":"2024-01-01T19:26:11.3023035Z","execution_finish_time":"2024-01-01T19:26:25.6897128Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"28b4e548-bfd9-4d90-9948-9c385e3ae284"},"text/plain":"StatementMeta(, b44d5d99-160a-4c42-b06b-f563272dffb5, 5, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"'\\n`Statsmodels` is a Python module that provides classes and functions for the estimation of many different statistical models\\nIt also allows you to conduct statistical tests and statistical data exploration.\\n'"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"babed862-7eb7-4253-a0b8-9252552f9dfe"},{"cell_type":"code","source":["\"\"\"\n","Define Variables\n","\"\"\"\n","\n","EXPERIMENT_NAME = \"EIA-forecasting-runs\"  # MLflow experiment name\n","\n","ts = time.time()\n","\n","\"\"\"\n","ML Flow: Machine Learning\n","\n","Autologging in Microsoft Fabric extends the MLflow autologging capabilities by automatically capturing the values of input parameters and output metrics of a machine learning model as it is being trained. This information is then logged to the workspace, where it can be accessed and visualized using the MLflow APIs or the corresponding experiment in the workspace. To learn more about autologging, see [Autologging in Microsoft Fabric](https://aka.ms/fabric-autologging).\n","\"\"\"\n","\n","mlflow.set_experiment(EXPERIMENT_NAME)\n","mlflow.autolog(disable=True)  # Disable MLflow autologging\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"b44d5d99-160a-4c42-b06b-f563272dffb5","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-01T19:25:49.0251681Z","session_start_time":null,"execution_start_time":"2024-01-01T19:26:26.1084817Z","execution_finish_time":"2024-01-01T19:26:27.6869312Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"734782f0-970f-49f4-8e03-2f92dcb79857"},"text/plain":"StatementMeta(, b44d5d99-160a-4c42-b06b-f563272dffb5, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1a1b555d-ddb2-4cf2-86d2-ea79277d456c"},{"cell_type":"markdown","source":["## Step 1: Datasets"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"13899506-0044-4b7b-842b-b670bb6b5d09"},{"cell_type":"markdown","source":["### Training Data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c44a0200-b7b7-4918-b8e6-43d7c047db43"},{"cell_type":"code","source":["\"\"\"\n","Step 1: Read the dataset from the lakehouse\n","\"\"\"\n","\n","df = spark.sql(\n","\"\"\"\n","SELECT \n","Type.FuelClass Fuel_class\n",",Ops.date_full Operating_date\n",",sum(Fact.NetGenerationElectricityMWh) Electricity_generated\n","FROM EIA_Lake.fact_generation Fact\n","left outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\n","left outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \n","where Ops.date_full > '1999-12-31'\n","GROUP BY \n","Type.FuelClass \n",",Ops.date_full \n","\"\"\"\n",")\n","\n","Training_data = df\n","\n","display(Training_data)\n","\n","\"\"\"\n","Data Reformatting\n","\"\"\"\n","Training_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\n","Training_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\n","Training_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\n","Training_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n","# Convert the PySpark DataFrame to a Pandas DataFrame\n","Training_data = Training_data.toPandas()\n","\n","\"\"\"\n","How to specify or convert the data types post-conversion in the Pandas DataFrame\n","\"\"\"\n","\n","# Assuming Training_data is your Pandas DataFrame\n","Training_data['Fuel class'] = Training_data['Fuel class'].astype(str)\n","# Assuming Training_data is your Pandas DataFrame\n","Training_data['Operating date'] = pd.to_datetime(Training_data['Operating date'])\n","# Convert the 'Electricity generated' column to int\n","Training_data['Electricity generated'] = Training_data['Electricity generated'].astype(int)\n","\n","\"\"\"\n","The dataset is structured on a daily basis\n","The goal is to develop a model to forecast the sales on a monthly basis, you need to resample on the column `Order Date`.\n","\n","1. group the `Furniture` category by `Order Date` and then \n","2. calculate the sum of the `Sales` column for each group in order to determine the total sales for each unique `Order Date`. \n","3. resample the `Sales` column using the `MS` frequency to aggregate the data by month and then you calculate the mean sales value for each month.\n","\"\"\"\n","\n","# Data preprocessing\n","Training_data = Training_data.sort_values('Operating date')\n","Training_data.isnull().sum()\n","Training_data = Training_data.groupby('Operating date')['Electricity generated'].sum().reset_index()\n","Training_data = Training_data.set_index('Operating date')\n","\n","\"\"\"\n","Add a docstring for exactly how this works. \n","Explain interpolation for example. \n","\"\"\"\n","# Assuming Historical_data is your DataFrame with a DateTime index\n","Training_data_resampled = Training_data['Electricity generated'].resample('MS').mean()\n","# Forward-fill missing values for the first few entries\n","Training_data_resampled_filled = Training_data_resampled.ffill()\n","# Interpolate missing values for the rest of the entries\n","Training_data_resampled_interpolated = Training_data_resampled_filled.interpolate(method='linear')\n","Training_data = Training_data_resampled_interpolated\n","\n","Training_data = Training_data.fillna(0) # Fill NaN values with zero\n","Training_data = Training_data.astype('int64')\n","Training_data = Training_data.reset_index()\n","Training_data['Operating date'] = pd.to_datetime(Training_data['Operating date'])\n","maximim_date = Training_data.reset_index()['Operating date'].max()\n","Training_data = Training_data[Training_data['Operating date'].dt.strftime('%Y-%m-%d') < \"2022-01-01\"] # training data\n","# Training_data.sort_index(inplace=True)\n","Training_data = Training_data.set_index(['Operating date'])\n","\n","print(Training_data)\n","display(maximim_date)\n","print(type(Training_data))\n","display(Training_data)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"b44d5d99-160a-4c42-b06b-f563272dffb5","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-01T19:25:49.0257034Z","session_start_time":null,"execution_start_time":"2024-01-01T19:26:28.3310449Z","execution_finish_time":"2024-01-01T19:26:54.0693399Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":19,"FAILED":0,"RUNNING":0},"jobs":[{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":31,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:52.390GMT","completionTime":"2024-01-01T19:26:52.505GMT","stageIds":[43],"jobGroup":"7","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":30,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:52.202GMT","completionTime":"2024-01-01T19:26:52.382GMT","stageIds":[42],"jobGroup":"7","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":29,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:51.493GMT","completionTime":"2024-01-01T19:26:52.196GMT","stageIds":[41],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","dataWritten":0,"dataRead":3077,"rowCount":32,"usageDescription":"","jobId":28,"name":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:50.988GMT","completionTime":"2024-01-01T19:26:51.081GMT","stageIds":[39,40],"jobGroup":"7","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","dataWritten":3077,"dataRead":0,"rowCount":42948,"usageDescription":"","jobId":27,"name":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:50.756GMT","completionTime":"2024-01-01T19:26:50.953GMT","stageIds":[38],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","dataWritten":0,"dataRead":1422,"rowCount":1,"usageDescription":"","jobId":24,"name":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:50.153GMT","completionTime":"2024-01-01T19:26:50.334GMT","stageIds":[34,35],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","dataWritten":0,"dataRead":2027,"rowCount":2,"usageDescription":"","jobId":23,"name":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:49.807GMT","completionTime":"2024-01-01T19:26:50.042GMT","stageIds":[33,32],"jobGroup":"7","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","dataWritten":0,"dataRead":1570,"rowCount":1,"usageDescription":"","jobId":22,"name":"toPandas at /tmp/ipykernel_5564/1533983620.py:33","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:49.300GMT","completionTime":"2024-01-01T19:26:49.545GMT","stageIds":[30,31],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":3077,"rowCount":32,"usageDescription":"","jobId":21,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:48.875GMT","completionTime":"2024-01-01T19:26:48.930GMT","stageIds":[28,29],"jobGroup":"7","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":3077,"dataRead":230916,"rowCount":42948,"usageDescription":"","jobId":20,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:48.023GMT","completionTime":"2024-01-01T19:26:48.810GMT","stageIds":[27],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":1870,"rowCount":4,"usageDescription":"","jobId":17,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:46.222GMT","completionTime":"2024-01-01T19:26:47.108GMT","stageIds":[24,23],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":1870,"dataRead":2574,"rowCount":8,"usageDescription":"","jobId":16,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:45.750GMT","completionTime":"2024-01-01T19:26:45.834GMT","stageIds":[22],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":5666,"rowCount":8,"usageDescription":"","jobId":15,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:44.538GMT","completionTime":"2024-01-01T19:26:45.480GMT","stageIds":[20,21],"jobGroup":"7","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":5666,"dataRead":6915,"rowCount":16,"usageDescription":"","jobId":14,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:43.972GMT","completionTime":"2024-01-01T19:26:44.126GMT","stageIds":[19],"jobGroup":"7","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":3998,"rowCount":6,"usageDescription":"","jobId":13,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:42.245GMT","completionTime":"2024-01-01T19:26:43.467GMT","stageIds":[17,18],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":3998,"dataRead":5662,"rowCount":12,"usageDescription":"","jobId":12,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...: Filtering files for query","submissionTime":"2024-01-01T19:26:41.567GMT","completionTime":"2024-01-01T19:26:41.723GMT","stageIds":[16],"jobGroup":"7","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":2574,"rowCount":4,"usageDescription":"","jobId":11,"name":"toString at String.java:2951","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:40.998GMT","completionTime":"2024-01-01T19:26:41.058GMT","stageIds":[15],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":6915,"rowCount":8,"usageDescription":"","jobId":10,"name":"toString at String.java:2951","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:40.250GMT","completionTime":"2024-01-01T19:26:40.682GMT","stageIds":[14],"jobGroup":"7","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":5662,"rowCount":6,"usageDescription":"","jobId":9,"name":"toString at String.java:2951","description":"Job group for statement 7:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '1999-12-31'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nTraining_data = df\n\ndisplay(Training_data)\n\n\"\"\"\nData Reformatting\n\"\"\"\nTraining_data = Training_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nTraining_data = Training_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nTraining_data = Training_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nTraining_data = Training_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nTraining_data = Training_data.toPandas()\n\n\"\"\"\nHow to specify or convert the data types post-co...","submissionTime":"2024-01-01T19:26:39.027GMT","completionTime":"2024-01-01T19:26:39.454GMT","stageIds":[13],"jobGroup":"7","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0fe1de48-3f89-4223-903c-d29da2df0202"},"text/plain":"StatementMeta(, b44d5d99-160a-4c42-b06b-f563272dffb5, 7, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"2de5d4fb-55cb-4d30-a71c-21b1dc417595","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 2de5d4fb-55cb-4d30-a71c-21b1dc417595)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Empty DataFrame\nColumns: [Electricity generated]\nIndex: []\n"]},{"output_type":"display_data","data":{"text/plain":"Timestamp('2023-08-01 00:00:00')"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"f6e89c77-99d8-469f-bc76-a6e6a5a259bb","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, f6e89c77-99d8-469f-bc76-a6e6a5a259bb)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"3aec51bc-1f39-41f2-974c-0ce9cb9171e1"},{"cell_type":"markdown","source":["### Validation Data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4d2ce6a2-072c-4d85-9e54-8a58e7cd348a"},{"cell_type":"code","source":["\"\"\"\n","Step 1: Read the dataset from the lakehouse\n","\"\"\"\n","\n","df = spark.sql(\n","\"\"\"\n","SELECT \n","Type.FuelClass Fuel_class\n",",Ops.date_full Operating_date\n",",sum(Fact.NetGenerationElectricityMWh) Electricity_generated\n","FROM EIA_Lake.fact_generation Fact\n","left outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\n","left outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \n","where Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\n","GROUP BY \n","Type.FuelClass \n",",Ops.date_full \n","\"\"\"\n",")\n","\n","validation_data = df\n","\n","\"\"\"\n","Data Reformatting\n","\"\"\"\n","validation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\n","validation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\n","validation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\n","validation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n","# Convert the PySpark DataFrame to a Pandas DataFrame\n","validation_data = validation_data.toPandas()\n","\n","# Type conversion is your Pandas DataFrame\n","validation_data['Fuel class'] = validation_data['Fuel class'].astype(str)\n","validation_data['Operating date'] = pd.to_datetime(validation_data['Operating date'])\n","validation_data['Electricity generated'] = validation_data['Electricity generated'].astype(int)\n","\n","# Data preprocessing\n","validation_data = validation_data.sort_values('Operating date')\n","validation_data.isnull().sum()\n","validation_data = validation_data.groupby('Operating date')['Electricity generated'].sum().reset_index()\n","validation_data = validation_data.set_index('Operating date')\n","\n","print(validation_data)\n","display(validation_data)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"b44d5d99-160a-4c42-b06b-f563272dffb5","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-01T19:25:49.0262194Z","session_start_time":null,"execution_start_time":"2024-01-01T19:26:54.4970633Z","execution_finish_time":"2024-01-01T19:26:58.105445Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":8,"FAILED":0,"RUNNING":0},"jobs":[{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":41,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...","submissionTime":"2024-01-01T19:26:57.418GMT","completionTime":"2024-01-01T19:26:57.518GMT","stageIds":[55],"jobGroup":"8","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":40,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...","submissionTime":"2024-01-01T19:26:57.262GMT","completionTime":"2024-01-01T19:26:57.414GMT","stageIds":[54],"jobGroup":"8","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"getRowsInJsonString at Display.scala:452","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":39,"name":"getRowsInJsonString at Display.scala:452","description":"Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...","submissionTime":"2024-01-01T19:26:57.144GMT","completionTime":"2024-01-01T19:26:57.256GMT","stageIds":[53],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":38,"name":"","description":"Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...","submissionTime":"2024-01-01T19:26:57.096GMT","completionTime":"2024-01-01T19:26:57.096GMT","stageIds":[],"jobGroup":"8","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","dataWritten":0,"dataRead":0,"rowCount":42916,"usageDescription":"","jobId":37,"name":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","description":"Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...","submissionTime":"2024-01-01T19:26:56.942GMT","completionTime":"2024-01-01T19:26:57.090GMT","stageIds":[52],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","dataWritten":0,"dataRead":1422,"rowCount":1,"usageDescription":"","jobId":34,"name":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","description":"Delta: Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...: Filtering files for query","submissionTime":"2024-01-01T19:26:56.192GMT","completionTime":"2024-01-01T19:26:56.487GMT","stageIds":[48,49],"jobGroup":"8","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","dataWritten":0,"dataRead":2027,"rowCount":2,"usageDescription":"","jobId":33,"name":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","description":"Delta: Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...: Filtering files for query","submissionTime":"2024-01-01T19:26:55.809GMT","completionTime":"2024-01-01T19:26:55.983GMT","stageIds":[46,47],"jobGroup":"8","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","dataWritten":0,"dataRead":1570,"rowCount":1,"usageDescription":"","jobId":32,"name":"toPandas at /tmp/ipykernel_5564/2607571173.py:31","description":"Delta: Job group for statement 8:\n\"\"\"\nStep 1: Read the dataset from the lakehouse\n\"\"\"\n\ndf = spark.sql(\n\"\"\"\nSELECT \nType.FuelClass Fuel_class\n,Ops.date_full Operating_date\n,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\nFROM EIA_Lake.fact_generation Fact\nleft outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\nleft outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \nwhere Ops.date_full > '2021-12-31' and Ops.date_full < '2023-01-01'\nGROUP BY \nType.FuelClass \n,Ops.date_full \n\"\"\"\n)\n\nvalidation_data = df\n\n\"\"\"\nData Reformatting\n\"\"\"\nvalidation_data = validation_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\nvalidation_data = validation_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\nvalidation_data = validation_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\nvalidation_data = validation_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n# Convert the PySpark DataFrame to a Pandas DataFrame\nvalidation_data = validation_data.toPandas()\n\n# Type conversion is ...: Filtering files for query","submissionTime":"2024-01-01T19:26:55.416GMT","completionTime":"2024-01-01T19:26:55.592GMT","stageIds":[45,44],"jobGroup":"8","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"bb51599b-2bb0-4fe6-a86c-bf283da85284"},"text/plain":"StatementMeta(, b44d5d99-160a-4c42-b06b-f563272dffb5, 8, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Empty DataFrame\nColumns: [Electricity generated]\nIndex: []\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"85089136-dbf6-4b2f-8e5a-2bc5a9e3242f","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 85089136-dbf6-4b2f-8e5a-2bc5a9e3242f)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"26beae8e-5ed6-4524-adfb-d45ed3a7c043"},{"cell_type":"markdown","source":["## Step 2: Statistical Analysis"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"add9da89-890e-495b-ad8b-d7bc97c517c5"},{"cell_type":"code","source":["\"\"\"\n","Statistical analysis\n","\n","A time series tracks four data elements at set intervals in order to determine the variation of those four elements in the time series pattern. These elements include:\n","\n","- **Level:** Refers to the fundamental component that represents the average value for a specific time period.\n","\n","- **Trend:** Describes whether the time series is decreasing, constant, or increasing over time.\n","\n","- **Seasonality:** Describes the periodic signal in the time series and looks for cyclic occurrences that affect the time series' increasing or decreasing patterns.\n","\n","- **Noise/Residual:** Refers to the random fluctuations and variability in the time series data that cannot be explained by the model.\n","\n","In the following, you will observe the above four components for your dataset after the pre-processing.\n","\"\"\"\n","\n","\n","# Decompose the time series into its components using statsmodels\n","result = sm.tsa.seasonal_decompose(Training_data, model='additive')\n","\n","# Labels and corresponding data for plotting\n","components = [('Seasonality', result.seasonal),\n","              ('Trend', result.trend),\n","              ('Residual', result.resid),\n","              ('Observed Data', Training_data)]\n","\n","# Create subplots in a grid\n","fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(12, 7))\n","plt.subplots_adjust(hspace=0.8)  # Adjust vertical space\n","axes = axes.ravel()\n","\n","# Plot the components\n","for ax, (label, data) in zip(axes, components):\n","    ax.plot(data, label=label, color='blue' if label != 'Observed Data' else 'purple')\n","    ax.set_xlabel('Time')\n","    ax.set_ylabel(label)\n","    ax.set_xlabel('Time', fontsize=10)\n","    ax.set_ylabel(label, fontsize=10)\n","    ax.legend(fontsize=10)\n","\n","plt.show()\n","\n","\"\"\"\n","Doing time series analysis. We are preforming seasonal decomposition. This gives an overall understanding of how these components contribute to the entire time series.\n","Understanding the seasonality, trend, and noise in the forecasting data through the above plots allows to capture underlying patterns, and develop models that make more accurate predictions that are resilient to random fluctuations.\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f6ccde91-f2a9-4c8f-bfec-2616ae37a152"},{"cell_type":"code","source":["\"\"\"\n","Statistical Measure of accuracy\n","\"\"\"\n","def mean_absolute_error(y_true, y_pred):\n","    \"\"\"\n","    Calculate Mean Absolute Error (MAE)\n","    \n","    Args:\n","    y_true: List or array containing true values\n","    y_pred: List or array containing predicted values\n","    \n","    Returns:\n","    mae: Mean Absolute Error\n","    \"\"\"\n","    if len(y_true) != len(y_pred):\n","        raise ValueError(\"Lengths of y_true and y_pred must be the same.\")\n","    \n","    total_error = 0\n","    for true_val, pred_val in zip(y_true, y_pred):\n","        total_error += abs(true_val - pred_val)\n","    \n","    mae = total_error / len(y_true)\n","    return mae"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1027f462-bc14-43c3-aa03-ba057638672c"},{"cell_type":"markdown","source":["## Step 3: Model Training and Tracking"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0b3a5d90-b0a4-45e6-8b44-7a50b88bc2bf"},{"cell_type":"markdown","source":["With your data in place, you can define the forecasting model. Apply the Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX) in this notebook. SARIMAX is a time series forecasting model that extends SARIMA to include exogenous variables. It combines autoregressive (AR) and moving average (MA) components, seasonal differencing, and external predictors to make accurate and flexible forecasts for time series data, making it a powerful tool for various forecasting tasks.\n","\n","You will also use MLfLow and Fabric Autologging to track the experiments. Here you'll load the delta table from the lakehouse. You may use other delta tables considering the lakehouse as the source."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"06b87879-bd4c-4fc8-a6e7-74ddb934ba5a"},{"cell_type":"markdown","source":["\"\"\"\n","SARIMAX, or Seasonal Autoregressive Integrated Moving Average with Exogenous Factors, \n","a time series forecasting method that extends the traditional ARIMA model to include seasonal components and exogenous variables. \n","\n","The acronym breaks down as follows:\n","- Seasonal: Accounts for patterns that repeat over known intervals or seasons.\n","- ARIMA: Stands for Autoregressive Integrated Moving Average, a model used for understanding and forecasting time series data.\n","- X: Denotes the inclusion of exogenous variables, which are external factors that can influence the time series being forecasted.\n","\n","In SARIMAX models, the seasonal and non-seasonal components of the time series data are modeled separately. \n","It's a powerful tool for handling time series data that exhibits both seasonal patterns and relationships with other variables.\n","It's often used\n","\"\"\""],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8b071033-0507-4874-b4d2-f56ee98390cf"},{"cell_type":"markdown","source":["### Hyperparameter tuning\n","\n","SARIMAX takes into account the parameters involved in regular ARIMA mode `(p,d,q)` and also adds the seasonality parameters `(P,D,Q,s)`. These arguments to SARIMAX model are called order `(p,d,q)` and seasonal order `(P,D,Q,s)` respectively and hence 7 parameters to tune. Prior to model training, you need to set up these parameters which are defined in the following.\n","\n","#### Order Parameters `(p, d, q)`:\n","- `p`: The order of the autoregressive (AR) component, indicating how many past observations are considered. It is also known as the AR order.\n","- `d`: The degree of differencing required to make the time series stationary. It is also known as the differencing order.\n","- `q`: The order of the moving average (MA) component, indicating how many past white noise error terms are considered. It is also known as the MA order.\n","\n","#### Seasonal Order Parameters `(P, D, Q, s)`:\n","\n","- `P`: The seasonal order of the autoregressive (AR) component, similar to `p` but for the seasonal part.\n","- `D`: The seasonal order of differencing, similar to `d` but for the seasonal part.\n","- `Q`: The seasonal order of the moving average (MA) component, similar to `q` but for the seasonal part.\n","- `s`: The number of time steps per seasonal cycle (e.g., 12 for monthly data with a yearly seasonality).\n","\n","\n"," The autoregressive order `p` represents the number of past observations in the time series that are used to predict the current value. Typically, `p` should be a non-negative integer. Common values for `p` are usually in the range of 0 to 3, although higher values are possible depending on the specific characteristics of the data. A higher p indicates a longer memory of past values in the model.\n","\n"," The moving average order `q` represents the number of past white noise error terms that are used to predict the current value. Similar to `p`, `q` should also be a non-negative integer. Common values for `q` are typically in the range of 0 to 3, but higher values may be necessary for certain time series. A higher `q` indicates a stronger reliance on past error terms to make predictions.\n","\n"," The differencing order `d` represents the number of times the time series needs to be differenced to achieve stationarity. `d` should be a non-negative integer. Common values for `d` are usually in the range of 0 to 2. A `d` value of 0 means the time series is already stationary, while higher values indicate the number of differencing operations required to make it stationary.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0451e60b-3530-40fe-8453-1ab7672ca72a"},{"cell_type":"markdown","source":["#### Determine Optional Parameters:\n","- The `enforce_stationarity` parameter controls whether or not the model should enforce stationarity on the time series data before fitting the SARIMAX model. When `enforce_stationarity` is set to `True` (the default), it indicates that the SARIMAX model should enforce stationarity on the time series data. This means that the SARIMAX model will automatically apply differencing to the data to make it stationary, as specified by the `d` and `D` orders, before fitting the model. This is a common practice because many time series models, including SARIMAX, assume that the data is stationary. If your time series is non-stationary (e.g., it exhibits trends or seasonality), it is generally a good practice to set `enforce_stationarity` to `True` and let the SARIMAX model handle the differencing to achieve stationarity. If your time series is already stationary (e.g., it has no trends or seasonality), you can set `enforce_stationarity` to `False` to avoid unnecessary differencing.\n","\n","- The `enforce_invertibility` parameter controls whether or not the model should enforce invertibility on the estimated parameters during the optimization process. When `enforce_invertibility` is set to `True` (the default), it indicates that the SARIMAX model should enforce invertibility on the estimated parameters. Invertibility ensures that the model is well-defined and that the estimated autoregressive (AR) and moving average (MA) coefficients are within the range of stationarity. Enforcing invertibility is typically recommended to ensure that the SARIMAX model adheres to the theoretical requirements for a stable time series model and helps prevent issues with model estimation and stability.\n","\n","- The default is an `AR(1)` model which refers to `(1,0,0)`. However, keep in mind that the appropriate values for `p`, `q`, and `d` can vary from one time series to another, and determining the optimal values often involves analyzing the autocorrelation and partial autocorrelation functions (ACF and PACF) of the time series data and using model selection criteria like AIC or BIC. It's common practice to try different combinations of p, q, and d and evaluate the model's performance for a given dataset. Note that the parameters for the seasonal order `(P, D, Q, s)` is similar in concept to the non-seasonal order parameters `(p, q, d)`, hence it is avoided explaining in detail again. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1e6bf261-af73-49bb-b34b-b6d574d0a158"},{"cell_type":"code","source":["def fetch_logged_data(run_id):\n","    client = MlflowClient()\n","    data = client.get_run(run_id).data\n","    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n","    # artifacts = [f.path for f in client.list_artifacts(run_id, \"model\")]\n","    return data.params, data.metrics, tags"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8c959bc2-e303-4435-a9d7-31f5e9b85ce9"},{"cell_type":"code","source":["def check_stationarity(data):\n","    result = adfuller(data)\n","    print('ADF Statistic:', result[0])\n","    print('p-value:', result[1])\n","    print('Critical Values:')\n","    for key, value in result[4].items():\n","        print(f'\\t{key}: {value}')\n","    if result[1] <= 0.05:\n","        print(\"Strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n","    else:\n","        print(\"Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n","\n","# Call the function with your time series data 'y'\n","check_stationarity(Training_data)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"acc6fe54-5d98-41be-a027-6f5fb89a2d19"},{"cell_type":"markdown","source":["#### Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) Method"],"metadata":{"nteract":{"transient":{"deleting":false}},"jp-MarkdownHeadingCollapsed":true},"id":"41eaf5d5-11ec-4ebe-b091-06d62910873b"},{"cell_type":"markdown","source":["##### Find Difference Values"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0bed464c-0f10-4913-a1b2-a0b46d127887"},{"cell_type":"code","source":["\n","# Original time series data\n","original_data = Training_data['Electricity generated']\n","\n","# First-order differences\n","first_differences = [original_data[i] - original_data[i - 1] for i in range(1, len(original_data))]\n","\n","# Second-order differences\n","second_differences = [original_data[i] - original_data[i - 2] for i in range(2, len(original_data))]\n","\n","# Third-order differences\n","third_differences = [original_data[i] - original_data[i - 3] for i in range(3, len(original_data))]\n","\n","# Calculating means for each differenced series\n","first_diff_mean = np.mean(first_differences)\n","second_diff_mean = np.mean(second_differences)\n","third_diff_mean = np.mean(third_differences)\n","\n","# Plotting the original data\n","plt.figure(figsize=(18, 6))\n","\n","# Plotting the original data\n","plt.subplot(141)\n","plt.plot(original_data, marker='o', color='blue')\n","plt.title('Original Time Series')\n","plt.xlabel('Index')\n","plt.ylabel('Value')\n","\n","# Plotting the first-order differences\n","plt.subplot(142)\n","plt.plot(first_differences, marker='o', color='red')\n","plt.axhline(y=first_diff_mean, color='black', linestyle='--', label=f'Mean: {first_diff_mean:.2f}')\n","plt.title('First-order Differences (d=1)')\n","plt.xlabel('Index')\n","plt.ylabel('Differences')\n","plt.legend()\n","\n","# Plotting the second-order differences\n","plt.subplot(143)\n","plt.plot(second_differences, marker='o', color='green')\n","plt.axhline(y=second_diff_mean, color='black', linestyle='--', label=f'Mean: {second_diff_mean:.2f}')\n","plt.title('Second-order Differences (d=2)')\n","plt.xlabel('Index')\n","plt.ylabel('Differences')\n","plt.legend()\n","\n","# Plotting the third-order differences\n","plt.subplot(144)\n","plt.plot(third_differences, marker='o', color='orange')\n","plt.axhline(y=third_diff_mean, color='black', linestyle='--', label=f'Mean: {third_diff_mean:.2f}')\n","plt.title('Third-order Differences (d=3)')\n","plt.xlabel('Index')\n","plt.ylabel('Differences')\n","plt.legend()\n","plt.show()\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fff7fd82-c2cc-4d66-ab41-612f8d5a564c"},{"cell_type":"code","source":["# Assuming 'first_differences', 'second_differences', 'third_differences' contain the respective differenced series\n","\n","# Create timestamp indices for the differenced series (assuming monthly data)\n","start_date = pd.to_datetime('2000-03-01')\n","end_date = pd.to_datetime('2021-12-01')  # Adjust based on your data length\n","timestamp1 = pd.date_range(start=pd.to_datetime('2000-01-01'), end=end_date, freq='M')\n","timestamp2 = pd.date_range(start=pd.to_datetime('2000-02-01'), end=end_date, freq='M')\n","timestamp3 = pd.date_range(start=pd.to_datetime('2000-03-01'), end=end_date, freq='M')\n","# Create pandas Series with timestamps for each differenced series\n","differenced_series1 = pd.Series(first_differences, index=timestamp1)\n","differenced_series2 = pd.Series(second_differences, index=timestamp2)\n","differenced_series3 = pd.Series(third_differences, index=timestamp3)\n","\n","# Calculating means for each differenced series\n","first_diff_mean = np.mean(differenced_series1)\n","second_diff_mean = np.mean(differenced_series2)\n","third_diff_mean = np.mean(differenced_series3)\n","\n","# Decompose each time series into its components using statsmodels\n","result1 = sm.tsa.seasonal_decompose(differenced_series1, model='additive')\n","result2 = sm.tsa.seasonal_decompose(differenced_series2, model='additive')\n","result3 = sm.tsa.seasonal_decompose(differenced_series3, model='additive')\n","\n","# Labels and corresponding data for plotting for each series\n","components1 = [('Seasonality', result1.seasonal),\n","               ('Trend', result1.trend),\n","               ('Residual', result1.resid),\n","               ('Observed Data', differenced_series1)]\n","\n","components2 = [('Seasonality', result2.seasonal),\n","               ('Trend', result2.trend),\n","               ('Residual', result2.resid),\n","               ('Observed Data', differenced_series2)]\n","\n","components3 = [('Seasonality', result3.seasonal),\n","               ('Trend', result3.trend),\n","               ('Residual', result3.resid),\n","               ('Observed Data', differenced_series3)]\n","\n","# Create subplots in a grid for each series\n","fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15, 10))\n","plt.subplots_adjust(hspace=0.8)  # Adjust vertical space\n","axes = axes.ravel()\n","\n","# Plot the components for each series\n","for i, (label, data) in enumerate(components1):\n","    ax = axes[i]\n","    ax.plot(data, label=label, color='blue' if label != 'Observed Data' else 'purple')\n","    ax.axhline(y=first_diff_mean, color='black', linestyle='--', label=f'Mean: {first_diff_mean:.2f}')\n","    ax.set_xlabel('Time')\n","    ax.set_ylabel(label)\n","    ax.set_xlabel('Time', fontsize=10)\n","    ax.set_ylabel(label, fontsize=10)\n","    ax.legend(fontsize=8)\n","\n","for i, (label, data) in enumerate(components2):\n","    ax = axes[i + 4]\n","    ax.plot(data, label=label, color='green' if label != 'Observed Data' else 'purple')\n","    ax.axhline(y=second_diff_mean, color='black', linestyle='--', label=f'Mean: {second_diff_mean:.2f}')\n","    ax.set_xlabel('Time')\n","    ax.set_ylabel(label)\n","    ax.set_xlabel('Time', fontsize=10)\n","    ax.set_ylabel(label, fontsize=10)\n","    ax.legend(fontsize=8)\n","\n","for i, (label, data) in enumerate(components3):\n","    ax = axes[i + 8]\n","    ax.plot(data, label=label, color='orange' if label != 'Observed Data' else 'purple')\n","    ax.axhline(y=third_diff_mean, color='black', linestyle='--', label=f'Mean: {third_diff_mean:.2f}')\n","    ax.set_xlabel('Time')\n","    ax.set_ylabel(label)\n","    ax.set_xlabel('Time', fontsize=10)\n","    ax.set_ylabel(label, fontsize=10)\n","    ax.legend(fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"e0edce17-e52e-46ce-a15d-6867f5e521f2"},{"cell_type":"code","source":["check_stationarity(differenced_series1)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"85ab7dbc-f3d3-4c46-a9cb-34ddcb85730f"},{"cell_type":"markdown","source":["#### Grid search method."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fb80d3bf-7ab6-4be0-9cd8-a27876c916ab"},{"cell_type":"code","source":["mlflow.autolog(disable=True)  # Disable MLflow autologging\n","\n","# Grid search method.\n","\"\"\"\n","The auto_arima function itself operates a bit like a grid search, \n","in that it tries various sets of p and q (also P and Q for seasonal models) parameters, \n","selecting the model that minimizes the AIC (or BIC, or whatever information criterion you select). \n","To select the differencing terms, auto_arima uses a test of stationarity (such as an augmented Dickey-Fuller test)\n"," and seasonality (such as the Canova-Hansen test) for seasonal models.\n"," \"\"\"\n","\n","\n","\n","# fit stepwise auto-ARIMA\n","stepwise_fit = pm.auto_arima(Training_data, start_p=1, start_q=1,\n","                             max_p=7, max_q=7, m=12,\n","                             start_P=0, seasonal=True,\n","                             d=1, D=1, trace=True,\n","                             error_action='ignore',  # don't want to know if an order does not work\n","                             suppress_warnings=True,  # don't want convergence warnings\n","                             stepwise=True)  # set to stepwise\n","\n","\n","# After obtaining stepwise_fit using auto_arima\n","\n","# Accessing the summary() method to get the best model summary\n","best_model_summary = stepwise_fit.summary()\n","\n","spark = SparkSession.builder.appName(\"ARIMA_Parameters\").getOrCreate()\n","\n","# Extracting the best model string from the summary\n","best_model_str = str(best_model_summary)\n","best_model_str = best_model_str.split('\\n')[3].split(':')[1].strip()\n","\n","print(best_model_str)\n","\n","# # Define the pattern for extracting ARIMA parameters\n","# pattern = r'ARIMA\\((\\d+),(\\d+),(\\d+)\\)\\((\\d+),(\\d+),(\\d+)\\)\\[(\\d+)\\]'\n","\n","# Define the pattern for extracting ARIMA parameters\n","pattern = r'SARIMAX\\((\\d+), (\\d+), (\\d+)\\)x\\((\\d+), (\\d+), \\[(\\d+)\\], (\\d+)\\)'\n","\n","\n","# Using regexp_extract to extract the values\n","extracted_values = spark.createDataFrame([(best_model_str,)], ['model_string']).select(\n","    regexp_extract('model_string', pattern, 1).alias('p'),\n","    regexp_extract('model_string', pattern, 2).alias('d'),\n","    regexp_extract('model_string', pattern, 3).alias('q'),\n","    regexp_extract('model_string', pattern, 4).alias('P'),\n","    regexp_extract('model_string', pattern, 5).alias('D'),\n","    regexp_extract('model_string', pattern, 6).alias('Q'),\n","    regexp_extract('model_string', pattern, 7).alias('s')\n",")\n","\n","# Extracted values as strings\n","extracted_row = extracted_values.collect()[0]\n","print(extracted_values)\n","print(extracted_row)\n","p_str = extracted_row['p']\n","d_str = extracted_row['d']\n","q_str = extracted_row['q']\n","P_str = extracted_row['P']\n","D_str = extracted_row['D']\n","Q_str = extracted_row['Q']\n","s_str = extracted_row['s']\n","\n","# Convert the extracted strings to integers with error handling\n","def convert_to_int(val):\n","    try:\n","        return int(val) if val.strip() else None\n","    except ValueError:\n","        return None\n","\n","# Conversion with error handling\n","p = convert_to_int(p_str)\n","d = convert_to_int(d_str)\n","q = convert_to_int(q_str)\n","P = convert_to_int(P_str)\n","D = convert_to_int(D_str)\n","Q = convert_to_int(Q_str)\n","s = convert_to_int(s_str)\n","\n","# Check for negative differencing and non-integer values\n","if d is not None and (not isinstance(d, int) or d < 0):\n","    raise ValueError('Differencing must be a non-negative integer.')\n","\n","\n","mlflow.autolog(disable=False)  # Enable MLflow autologging\n","\n","# Start an MLflow run\n","with mlflow.start_run():\n","\n","    mod = sm.tsa.statespace.SARIMAX(Training_data,\n","                                order=(p, d, q),\n","                                seasonal_order=(P, D, Q, s),\n","                                enforce_stationarity=True,\n","                                enforce_invertibility=True)\n","    results = mod.fit(disp=False)\n","    # Forecasting on the validation dataset (2022)\n","    forecast = results.get_forecast(steps=len(validation_data))\n","\n","    # Compare forecast to actuals for evaluation\n","    forecast_values = forecast.predicted_mean\n","\n","    # Log SARIMAX model parameters\n","    mlflow.log_params({\"order\": (p, d, q), \"seasonal_order\": (P, D, Q, s), 'enforce_stationarity': True, 'enforce_invertibility': True})\n","\n","\n","# Log the model and parameters\n","model_name = f\"{EXPERIMENT_NAME}-Sarimax\"\n","with mlflow.start_run(run_name=\"Sarimax\") as run:\n","    mlflow.statsmodels.log_model(results,model_name,registered_model_name=model_name)\n","    mlflow.log_params({\"order\":(p,d,q),\"seasonal_order\":(P, D, Q, s),'enforce_stationarity':True,'enforce_invertibility':True})\n","    model_uri = f\"runs:/{run.info.run_id}/{model_name}\"\n","    print(\"Model saved in run %s\" % run.info.run_id)\n","    print(f\"Model URI: {model_uri}\")\n","mlflow.end_run()\n","\n","# Load the saved model\n","loaded_model = mlflow.statsmodels.load_model(model_uri)\n","\n","# Validation Steps\n","Future = pd.DataFrame(forecast_values).reset_index()\n","validation = pd.DataFrame(validation_data).reset_index()\n","Future['Operating date'] = pd.to_datetime(Future['index'])\n","# Using merge\n","result = pd.merge(validation, Future[['Operating date', 'predicted_mean']], on='Operating date', how='left')\n","result['Order'] = result.apply(lambda row: f\"SARIMAX({p}, {d}, {q})x({P}, {D}, {Q}, {s})\", axis=1)\n","result['MAPE'] = np.NAN\n","# Calculate the Mean Absolute Percentage Error (MAPE) between the 'Actual_Sales' and 'Forecasted_Sales' \n","result['MAPE'] = mean_absolute_percentage_error(result['Electricity generated'], result['predicted_mean']) * 100\n","result['MAE'] = mean_absolute_error(result['Electricity generated'], result['predicted_mean'])\n","result = result.reindex(columns=['Order','Operating date', 'Electricity generated', 'predicted_mean', 'MAPE', 'MAE'])\n","display(result)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"ef6f0ace-70c2-48f0-a462-3249b0359fbe"},{"cell_type":"code","source":["result.rename(columns={'Operating date': 'Operating_Date'}, inplace=True)\n","result.rename(columns={'Electricity generated': 'Electricity_generated'}, inplace=True)\n","result.rename(columns={'predicted_mean': 'Forecast_Electricity'}, inplace=True)\n","\n","# Write Back the results into the lakehouse\n","table_name = \"Grid_Forecast\"\n","spark.createDataFrame(result).write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"91e8ade0-05fc-4dcf-9048-b5326440364d"},{"cell_type":"markdown","source":["#### Best Fit method (Lowest MAPE & MAE)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5511689b-3069-4077-949f-0a6a7f2334db"},{"cell_type":"code","source":["p = d = q = range(0, 8)\n","\n","# Non-seasonal combinations\n","pdq_combinations = list(itertools.product(p, d, q))\n","num_pdq_combinations = len(pdq_combinations)\n","\n","# Seasonal combinations\n","seasonal_pdq_combinations = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n","num_seasonal_pdq_combinations = len(seasonal_pdq_combinations)\n","\n","# Total combinations\n","total_combinations = num_pdq_combinations * num_seasonal_pdq_combinations\n","\n","print(f\"Number of pdq combinations: {num_pdq_combinations}\")\n","print(f\"Number of seasonal_pdq combinations: {num_seasonal_pdq_combinations}\")\n","print(f\"Total number of combinations: {total_combinations}\")\n","print(\"need to optimize the search process or utilizing parallel computing can significantly reduce the time taken to find the best parameters for the model.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b1c9e087-7da3-4b7b-9ae7-79bda2fcdaa1"},{"cell_type":"code","source":["validation_data['Electricity generated'] = pd.to_numeric(validation_data['Electricity generated'], errors='coerce')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"42c8ba6a-8aec-4e71-83f7-9fc29f715db4"},{"cell_type":"code","source":["class ModelCombination:\n","    def __init__(self, param, param_seasonal):\n","        self.param = param\n","        self.param_seasonal = param_seasonal\n","        # self.aic = aic\n","        self.model = None\n","\n","def find_best_model(y):\n","    best_aic = np.inf\n","    best_params = None\n","    best_model = None\n","    lowest_mape = np.inf  # Initialize with a high value\n","    best_model_info = None  # Initialize as None\n","    # Define ranges for hyperparameters\n","    p = d = q = range(0, 5)\n","    pdq = list(itertools.product(p, d, q))\n","    seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n","    model_name = f\"{EXPERIMENT_NAME}-Sarimax\"\n","\n","    for param in pdq:\n","        for param_seasonal in seasonal_pdq:\n","            try:\n","                with mlflow.start_run():\n","                    mod = sm.tsa.statespace.SARIMAX(Training_data,\n","                                                    order=param,\n","                                                    seasonal_order=param_seasonal,\n","                                                    enforce_stationarity=True,\n","                                                    enforce_invertibility=True)\n","                    results = mod.fit(disp=False)\n","                    # Forecasting on the validation dataset (2022)\n","                    forecast = results.get_forecast(steps=len(validation_data))\n","\n","                    # Compare forecast to actuals for evaluation\n","                    forecast_values = forecast.predicted_mean\n","\n","                    best_params = (param, param_seasonal)\n","                    best_model = ModelCombination(param, param_seasonal)\n","                    best_model.model = results  # Storing the best model object\n","\n","                    # Log SARIMAX model parameters\n","                    mlflow.log_params({\"order\": best_model.param, \"seasonal_order\": best_model.param_seasonal, 'enforce_stationarity': True, 'enforce_invertibility': True})\n","                    mape = mean_absolute_percentage_error(validation_data['Electricity generated'], forecast_values) * 100\n","                    mae = mean_absolute_error(validation_data['Electricity generated'], forecast_values)\n","\n","                    # Check if the current MAPE is lower than the lowest recorded\n","                    if mape < lowest_mape:\n","                        lowest_mape = mape  # Update the lowest MAPE\n","                        best_model_info = {\n","                            'param': param,\n","                            'param_seasonal': param_seasonal,\n","                            'mape': lowest_mape,\n","                            'model_summary': best_model_str  # or any other relevant model info\n","                        }\n","\n","\n","                    mlflow.log_metric(\"MAPE\", mape)\n","                    mlflow.log_metric(\"MAE\", mae)\n","\n","                with mlflow.start_run(run_name=\"Sarimax\") as run:\n","                    mlflow.statsmodels.log_model(results,model_name,registered_model_name=model_name)\n","                    # mlflow.statsmodels.log_model(best_model.model, model_name, registered_model_name=model_name)\n","                    mlflow.log_params({\"order\": best_model.param, \"seasonal_order\": best_model.param_seasonal, 'enforce_stationarity': True, 'enforce_invertibility': True})\n","                    mlflow.log_metric(\"MAPE\", mape)\n","                    mlflow.log_metric(\"MAE\", mae)               \n","                    model_uri = f\"runs:/{run.info.run_id}/{model_name}\"\n","                    print(\"Best model saved in run %s\" % run.info.run_id)\n","                    print(f\"Best model URI: {model_uri}\")\n","                    mlflow.end_run()\n","                # Load the saved model\n","                loaded_model = mlflow.statsmodels.load_model(model_uri)\n","\n","                # Validation Steps\n","                Future = pd.DataFrame(forecast_values).reset_index()\n","                validation = pd.DataFrame(validation_data).reset_index()\n","                Future['Operating date'] = pd.to_datetime(Future['index'])\n","                # Using merge\n","                result = pd.merge(validation, Future[['Operating date', 'predicted_mean']], on='Operating date', how='left')\n","\n","                # Accessing the summary() method to get the best model summary\n","                best_model_summary = results.summary()\n","\n","\n","\n","                # Extracting the best model string from the summary\n","                best_model_str = str(best_model_summary)\n","                best_model_str = best_model_str.split('\\n')[3].split(':')[1].strip()\n","\n","         \n","                result['Order'] = best_model_str\n","                result['MAPE'] = np.NAN\n","                # Calculate the Mean Absolute Percentage Error (MAPE) between the 'Actual_Sales' and 'Forecasted_Sales' \n","                result['MAPE'] = mean_absolute_percentage_error(result['Electricity generated'], result['predicted_mean']) * 100\n","                result['MAE'] = mean_absolute_error(result['Electricity generated'], result['predicted_mean'])\n","\n","\n","                result = result.reindex(columns=['Order','Operating date', 'Electricity generated', 'predicted_mean', 'MAPE', 'MAE'])\n","                display(result)\n","\n","\n","                result.rename(columns={'Operating date': 'Operating_Date'}, inplace=True)\n","                result.rename(columns={'Electricity generated': 'Electricity_generated'}, inplace=True)\n","                result.rename(columns={'predicted_mean': 'Forecast_Electricity'}, inplace=True)\n","\n","                # Write Back the results into the lakehouse\n","                table_name = \"EIA_Test\"\n","                spark.createDataFrame(result).write.mode(\"append\").format(\"delta\").save(f\"Tables/{table_name}\")\n","                print(f\"Spark dataframe saved to delta table: {table_name}\")\n","            except Exception as e:\n","                print(f\"Model fitting failed for {param}, {param_seasonal}. Error: {str(e)}\")\n","                continue \n","                           \n","    # After all iterations are done, you can access the best model information\n","    if best_model_info is not None:\n","        print(\"Best model found with lowest MAPE:\")\n","        print(f\"Parameters: {best_model_info['param']}\")\n","        print(f\"Seasonal Parameters: {best_model_info['param_seasonal']}\")\n","        print(f\"Lowest MAPE: {best_model_info['mape']}\")\n","        print(f\"Model Summary: {best_model_info['model_summary']}\")\n","    else:\n","        print(\"No valid model found\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2ccba516-870d-4c28-864b-e14c10d1c655"},{"cell_type":"code","source":["find_best_model(Training_data)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"a8046661-2cea-4261-b541-27f717a24fc9"},{"cell_type":"markdown","source":["#### AIC (Akaike Information Criterion) method"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9825f78e-4e7d-444c-948e-4fb0fc8261b6"},{"cell_type":"code","source":["# Best version of AIC\n","\n","class ModelCombination:\n","    def __init__(self, param, param_seasonal, aic):\n","        self.param = param\n","        self.param_seasonal = param_seasonal\n","        self.aic = aic\n","        self.model = None\n","\n","def find_best_model(y):\n","    best_aic = np.inf\n","    best_params = None\n","    best_model = None\n","\n","    # Define ranges for hyperparameters\n","    p = d = q = range(0, 8)\n","    pdq = list(itertools.product(p, d, q))\n","    seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n","    model_name = f\"{EXPERIMENT_NAME}-Sarimax\"\n","\n","    for param in pdq:\n","        for param_seasonal in seasonal_pdq:\n","            try:\n","                mod = sm.tsa.statespace.SARIMAX(y,\n","                                                order=param,\n","                                                seasonal_order=param_seasonal,\n","                                                enforce_stationarity=True,\n","                                                enforce_invertibility=True)\n","                results = mod.fit(disp=False)\n","                aic = results.aic\n"," \n","                \n","\n","\n","                # Check if current AIC is the best so far\n","                if aic < best_aic:\n","                    best_aic = aic\n","                    best_params = (param, param_seasonal)\n","                    best_model = ModelCombination(param, param_seasonal, aic)\n","                    best_model.model = results  # Storing the best model object\n","            except Exception as e:\n","                print(f\"Model fitting failed for {param}, {param_seasonal}. Error: {str(e)}\")\n","                continue\n","\n","    # Log the best model and parameters using MLflow\n","    if best_model is not None and best_model.model is not None:\n","        with mlflow.start_run(run_name=\"Sarimax\") as run:\n","            mlflow.statsmodels.log_model(best_model.model, model_name, registered_model_name=model_name)\n","            mlflow.log_params({\"order\": best_model.param, \"seasonal_order\": best_model.param_seasonal, 'enforce_stationarity': True, 'enforce_invertibility': True})\n","            model_uri = f\"runs:/{run.info.run_id}/{model_name}\"\n","            print(\"Best model saved in run %s\" % run.info.run_id)\n","            print(f\"Best model URI: {model_uri}\")\n","            mlflow.end_run()\n","            return model_uri, best_model.model  \n","    else:\n","        print(\"No valid model found\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"176576c5-b08c-4260-8ce5-e60d0d6f8d6e"},{"cell_type":"code","source":["\"\"\"\n","# I need this part\n","model_uri, overall_results = find_best_model(Training_data)\n","model_uri = model_uri\n","results = overall_results\n","print(model_uri)\n","print(results.summary())\n","# Load the saved model\n","loaded_model = mlflow.statsmodels.load_model(model_uri)\n","\"\"\"\n"],"outputs":[],"execution_count":null,"metadata":{},"id":"2b02974a"},{"cell_type":"markdown","source":["#### Training Method I used (Manual)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b6e24d17-685e-4fdc-b23c-1200da864e26"},{"cell_type":"code","source":["p = 7\n","d = 1\n","q = 5\n","P = 1\n","D = 1\n","Q = 1\n","s = 12\n","\n","# Start an MLflow run\n","with mlflow.start_run():\n","\n","    mod = sm.tsa.statespace.SARIMAX(Training_data,\n","                                order=(p, d, q),\n","                                seasonal_order=(P, D, Q, s),\n","                                enforce_stationarity=True,\n","                                enforce_invertibility=True)\n","    results = mod.fit(disp=False)\n","    # Forecasting on the validation dataset (2022)\n","    forecast = results.get_forecast(steps=len(validation_data))\n","\n","    # Compare forecast to actuals for evaluation\n","    forecast_values = forecast.predicted_mean\n","\n","    # Log SARIMAX model parameters\n","    mlflow.log_params({\"order\": (p, d, q), \"seasonal_order\": (P, D, Q, s), 'enforce_stationarity': True, 'enforce_invertibility': True})\n","\n","\n","# Log the model and parameters\n","model_name = f\"{EXPERIMENT_NAME}-Sarimax\"\n","with mlflow.start_run(run_name=\"Sarimax\") as run:\n","    mlflow.statsmodels.log_model(results,model_name,registered_model_name=model_name)\n","    mlflow.log_params({\"order\":(p,d,q),\"seasonal_order\":(P, D, Q, s),'enforce_stationarity':True,'enforce_invertibility':True})\n","    model_uri = f\"runs:/{run.info.run_id}/{model_name}\"\n","    print(\"Model saved in run %s\" % run.info.run_id)\n","    print(f\"Model URI: {model_uri}\")\n","mlflow.end_run()\n","\n","# Load the saved model\n","loaded_model = mlflow.statsmodels.load_model(model_uri)\n","\n","# fetch logged data\n","params, metrics, tags = fetch_logged_data(run.info.run_id)\n","\n","pprint(params)\n","\n","pprint(metrics)\n","\n","pprint(tags)\n","\n","# pprint(artifacts)\n","\n","# Validation Steps\n","Future = pd.DataFrame(forecast_values).reset_index()\n","validation = pd.DataFrame(validation_data).reset_index()\n","Future['Operating date'] = pd.to_datetime(Future['index'])\n","# Using merge\n","result = pd.merge(validation, Future[['Operating date', 'predicted_mean']], on='Operating date', how='left')\n","result['Order'] = result.apply(lambda row: f\"SARIMAX({p}, {d}, {q})x({P}, {D}, {Q}, {s})\", axis=1)\n","result['MAPE'] = np.NAN\n","# Calculate the Mean Absolute Percentage Error (MAPE) between the 'Actual_Sales' and 'Forecasted_Sales' \n","result['MAPE'] = mean_absolute_percentage_error(result['Electricity generated'], result['predicted_mean']) * 100\n","result['MAE'] = mean_absolute_error(result['Electricity generated'], result['predicted_mean'])\n","result = result.reindex(columns=['Order','Operating date', 'Electricity generated', 'predicted_mean', 'MAPE', 'MAE'])\n","display(result)\n","\n","\n","result.rename(columns={'Operating date': 'Operating_Date'}, inplace=True)\n","result.rename(columns={'Electricity generated': 'Electricity_generated'}, inplace=True)\n","result.rename(columns={'predicted_mean': 'Forecast_Electricity'}, inplace=True)\n","\n","# Write Back the results into the lakehouse\n","table_name = \"EIA_Forecast\"\n","spark.createDataFrame(result).write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"8e5db883-0923-4de2-90aa-0e38e0702edd"},{"cell_type":"markdown","source":["## Step 4: Score the model and save predictions"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a4400024-f0ec-411c-8ee4-4a8c80f0b50c"},{"cell_type":"markdown","source":["Scoring a model typically refers to evaluating its performance or assessing how well it predicts or fits the data it was trained on or applied to. For instance, in the context of a SARIMAX (Seasonal Autoregressive Integrated Moving Average with Exogenous Factors) model, scoring could involve several evaluation metrics.\n","\n","Here are a few common ways to score a SARIMAX model:\n","\n","1. **Mean Squared Error (MSE):** Measures the average of the squared differences between predicted and actual values. Lower MSE indicates better performance.\n","\n","2. **Root Mean Squared Error (RMSE):** The square root of the MSE, giving an error value in the same units as the target variable. Lower RMSE signifies better performance.\n","\n","3. **Mean Absolute Error (MAE):** Measures the average of the absolute differences between predicted and actual values. Similar to MSE but less sensitive to outliers.\n","\n","4. **AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion):** These are used for model selection among a set of models. Lower AIC or BIC values indicate a better fit, with a balance between goodness of fit and model complexity.\n","\n","5. **R-squared (R²) or adjusted R-squared:** Measures the proportion of variance in the dependent variable that is predictable from the independent variables. Higher R² values indicate a better fit of the model to the data.\n","\n","When applying a SARIMAX model, you'd use historical data for training, leaving out a portion (the validation or test set) to evaluate its predictive performance. After fitting the model on the training data, you'd use the test set to generate predictions and then compare these predictions to the actual values to calculate these scoring metrics.\n","\n","These scores help to gauge how well the model is performing and whether it needs adjustments or fine-tuning to improve its predictions."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8fc226ef-d340-4e8d-9f47-e4846082ddfd"},{"cell_type":"markdown","source":["### Re-training and Prediction\n","\n","Re-training: Once validated, retrain the model on the entire dataset (training + validation) to utilize all available data.\n","Prediction: Use the retrained model to forecast future time points beyond the last known data point."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f411e34a-e66a-4f7a-9ba4-0ad318e50389"},{"cell_type":"code","source":["# df = spark.sql(\n","# \"\"\"\n","# SELECT \n","# Type.FuelClass Fuel_class\n","# ,Ops.date_full Operating_date\n","# ,sum(Fact.NetGenerationElectricityMWh) Electricity_generated\n","# FROM EIA_Lake.fact_generation Fact\n","# left outer join dim_fuel_type Type on Fact.FuelTypeKey = Type.FuelTypeKey\n","# left outer join dim_date Ops on Fact.SurveyDateKey = Ops.date_id \n","# where Ops.date_full > '1999-12-31'\n","# GROUP BY \n","# Type.FuelClass \n","# ,Ops.date_full \n","# \"\"\"\n","# )\n","\n","# Historical_data = df\n","\n","# \"\"\"\n","# Data Reformatting\n","# \"\"\"\n","# Historical_data = Historical_data.withColumnRenamed(\"Fuel_Class\", \"Fuel class\")\n","# Historical_data = Historical_data.withColumnRenamed(\"Operating_Date\", \"Operating date\")\n","# Historical_data = Historical_data.withColumnRenamed(\"Electricity_Generated\", \"Electricity generated\")\n","# Historical_data = Historical_data.withColumn(\"Operating date\", to_date(col(\"Operating date\")))\n","# # Convert the PySpark DataFrame to a Pandas DataFrame\n","# Historical_data = Historical_data.toPandas()\n","\n","# \"\"\"\n","# How to specify or convert the data types post-conversion in the Pandas DataFrame\n","# \"\"\"\n","\n","# # Assuming Historical_data is your Pandas DataFrame\n","# Historical_data['Fuel class'] = Historical_data['Fuel class'].astype(str)\n","# # Assuming Historical_data is your Pandas DataFrame\n","# Historical_data['Operating date'] = pd.to_datetime(Historical_data['Operating date'])\n","# # Convert the 'Electricity generated' column to int\n","# Historical_data['Electricity generated'] = Historical_data['Electricity generated'].astype(int)\n","\n","# \"\"\"\n","# The dataset is structured on a daily basis\n","# The goal is to develop a model to forecast the sales on a monthly basis, you need to resample on the column `Order Date`.\n","\n","# 1. group the `Furniture` category by `Order Date` and then \n","# 2. calculate the sum of the `Sales` column for each group in order to determine the total sales for each unique `Order Date`. \n","# 3. resample the `Sales` column using the `MS` frequency to aggregate the data by month and then you calculate the mean sales value for each month.\n","# \"\"\"\n","\n","# # Data preprocessing\n","# Historical_data = Historical_data.sort_values('Operating date')\n","# Historical_data.isnull().sum()\n","# Historical_data = Historical_data.groupby('Operating date')['Electricity generated'].sum().reset_index()\n","# Historical_data = Historical_data.set_index('Operating date')\n","\n","# # Assuming Historical_data is your DataFrame with a DateTime index\n","# Historical_data_resampled = Historical_data['Electricity generated'].resample('MS').mean()\n","# # Forward-fill missing values for the first few entries\n","# Historical_data_resampled_filled = Historical_data_resampled.ffill()\n","# # Interpolate missing values for the rest of the entries\n","# Historical_data_resampled_interpolated = Historical_data_resampled_filled.interpolate(method='linear')\n","# Historical_data = Historical_data_resampled_interpolated\n","# Historical_data = Historical_data.fillna(0)  # Fill NaN values with zero\n","# Historical_data = Historical_data.astype('int64')\n","# Historical_data = Historical_data.reset_index()\n","# Historical_data['Operating date'] = pd.to_datetime(Historical_data['Operating date'])\n","# # Historical_data.sort_index(inplace=True)\n","# Historical_data = Historical_data.set_index(['Operating date'])\n","\n","# p = 7\n","# d = 1\n","# q = 5\n","# P = 1\n","# D = 1\n","# Q = 1\n","# s = 12\n","# # Log the model and parameters\n","# model_name = f\"{EXPERIMENT_NAME}-Sarimax\"\n","\n","# # Start an MLflow run\n","# with mlflow.start_run():\n","\n","#     mod = sm.tsa.statespace.SARIMAX(Training_data, order=(p, d, q), seasonal_order=(P, D, Q, s),enforce_stationarity=True,enforce_invertibility=True)\n","\n","#     results = mod.fit(disp=False)\n","#     # This initiates the forecast one month after maximim_date and for the next 6 months (months=6)\n","#     forecast = results.get_prediction(start=maximim_date + pd.DateOffset(months=1), end=maximim_date + pd.DateOffset(months=12), dynamic=False)\n","\n","#     # Compare forecast to actuals for evaluation\n","#     forecast_values = forecast.predicted_mean\n","\n","#     # Log SARIMAX model parameters\n","#     mlflow.log_params({\"order\": (p, d, q), \"seasonal_order\": (P, D, Q, s), 'enforce_stationarity': True, 'enforce_invertibility': True})\n","\n","\n","# with mlflow.start_run(run_name=\"Sarimax\") as run:\n","#     mlflow.statsmodels.log_model(results, model_name, registered_model_name=model_name)\n","#     mlflow.log_params({\"order\": (p, d, q), \"seasonal_order\": (P, D, Q, s), 'enforce_stationarity': True,'enforce_invertibility': True})\n","#     model_uri = f\"runs:/{run.info.run_id}/{model_name}\"\n","#     print(\"Model saved in run %s\" % run.info.run_id)\n","#     print(f\"Model URI: {model_uri}\")\n","# mlflow.end_run()\n","\n","# # Load the saved model\n","# loaded_model = mlflow.statsmodels.load_model(model_uri)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"19b94e8b-c6e7-4c11-9ef6-c39921d9f2f6"},{"cell_type":"code","source":["\n","# # Validation Steps\n","# Future = pd.DataFrame(forecast_values).reset_index()\n","# Historical_data = pd.DataFrame(Historical_data).reset_index()\n","# Future['Operating date'] = pd.to_datetime(Future['index'])\n","\n","# Historical_data['Order'] = Future.apply(lambda row: f\"SARIMAX({p}, {d}, {q})x({P}, {D}, {Q}, {s})\", axis=1)\n","# Historical_data['MAPE'] = np.NAN\n","# Historical_data['MAE'] = np.NAN\n","# Future['Order'] = Future.apply(lambda row: f\"SARIMAX({p}, {d}, {q})x({P}, {D}, {Q}, {s})\", axis=1)\n","# Future['MAPE'] = np.NAN\n","# Future['MAE'] = np.NAN\n","\n","# Historical_data.rename(columns={'Electricity generated': 'Electricity_generated'}, inplace=True)\n","# Historical_data.rename(columns={'Operating date': 'Operating_Date'}, inplace=True)\n","# Historical_data = Historical_data.reindex(columns=['Order','Operating_Date', 'Electricity_generated', 'MAPE', 'MAE'])\n","# Future.rename(columns={'Operating date': 'Operating_Date'}, inplace=True)\n","# # Future.rename(columns={'Electricity generated': 'Electricity_generated'}, inplace=True)\n","# Future.rename(columns={'predicted_mean': 'Electricity_generated'}, inplace=True)\n","# Future = Future.reindex(columns=['Order','Operating_Date', 'Electricity_generated', 'MAPE', 'MAE'])\n","\n","# display(Historical_data)\n","# display(Future)\n","\n","\n","# final_result = pd.concat([Historical_data, Future])\n","# final_result = final_result.fillna(0) # Fill NaN values with zero\n","# final_result['Electricity_generated'] = final_result['Electricity_generated'].astype(int)\n","# final_result['MAPE'] = final_result['MAPE'].astype(int)\n","# final_result['MAE'] = final_result['MAE'].astype(int)\n","\n","\n","# display(final_result)\n","\n","# # Write Back the results into the lakehouse\n","# table_name = \"Demand_Forecast_New_1\"\n","# spark.createDataFrame(final_result).write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","# print(f\"Spark dataframe saved to delta table: {table_name}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"9ae60e96-d363-414c-907b-93bf2e061dc2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"host":{"synapse_widget":{"token":"9a4035e2-8953-42a5-8f4b-fde058fe49d7","state":{"2de5d4fb-55cb-4d30-a71c-21b1dc417595":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"Fossil Fuels","1":"2023-07-01","2":"283641221.660","index":1},{"0":"All Other Fuels","1":"2023-07-01","2":"71255780.400","index":2},{"0":"Unknown","1":"2023-07-01","2":"0.000","index":3},{"0":"Fossil Fuels","1":"2023-03-01","2":"185806050.250","index":4},{"0":"Fossil Fuels","1":"2023-04-01","2":"163770399.410","index":5},{"0":"All Other Fuels","1":"2023-06-01","2":"66296171.490","index":6},{"0":"Unknown","1":"2023-02-01","2":"0.000","index":7},{"0":"All Other Fuels","1":"2023-05-01","2":"62806718.530","index":8},{"0":"All Other Fuels","1":"2023-03-01","2":"64081044.870","index":9},{"0":"Unknown","1":"2023-08-01","2":"0.000","index":10},{"0":"Unknown","1":"2023-06-01","2":"0.000","index":11},{"0":"Renewable Fuels","1":"2023-03-01","2":"81306140.110","index":12},{"0":"Renewable Fuels","1":"2023-07-01","2":"72308202.330","index":13},{"0":"Renewable Fuels","1":"2023-08-01","2":"71901254.550","index":14},{"0":"Unknown","1":"2023-04-01","2":"0.000","index":15},{"0":"Fossil Fuels","1":"2023-06-01","2":"223180548.640","index":16},{"0":"Unknown","1":"2023-03-01","2":"0.000","index":17},{"0":"Renewable Fuels","1":"2023-01-01","2":"74377626.960","index":18},{"0":"Renewable Fuels","1":"2023-05-01","2":"81016963.220","index":19},{"0":"Renewable Fuels","1":"2023-02-01","2":"74331464.980","index":20},{"0":"Fossil Fuels","1":"2023-01-01","2":"202511960.840","index":21},{"0":"All Other Fuels","1":"2023-02-01","2":"62028959.610","index":22},{"0":"Unknown","1":"2023-01-01","2":"0.000","index":23},{"0":"All Other Fuels","1":"2023-01-01","2":"72244265.880","index":24},{"0":"Fossil Fuels","1":"2023-08-01","2":"282643683.010","index":25},{"0":"Fossil Fuels","1":"2023-05-01","2":"185311022.090","index":26},{"0":"Renewable Fuels","1":"2023-04-01","2":"79421669.100","index":27},{"0":"Unknown","1":"2023-05-01","2":"0.000","index":28},{"0":"All Other Fuels","1":"2023-04-01","2":"57811652.700","index":29},{"0":"Renewable Fuels","1":"2023-06-01","2":"68829493.060","index":30},{"0":"All Other Fuels","1":"2023-08-01","2":"71041584.420","index":31},{"0":"Fossil Fuels","1":"2023-02-01","2":"173950992.140","index":32}],"schema":[{"key":"0","name":"Fuel_class","type":"string"},{"key":"1","name":"Operating_date","type":"date"},{"key":"2","name":"Electricity_generated","type":"decimal"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"f6e89c77-99d8-469f-bc76-a6e6a5a259bb":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[],"schema":[{"key":"0","name":"Electricity generated","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"85089136-dbf6-4b2f-8e5a-2bc5a9e3242f":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[],"schema":[{"key":"0","name":"Electricity generated","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"2de5d4fb-55cb-4d30-a71c-21b1dc417595":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"Fossil Fuels","1":"2023-07-01","2":"283641221.660","index":1},{"0":"All Other Fuels","1":"2023-07-01","2":"71255780.400","index":2},{"0":"Unknown","1":"2023-07-01","2":"0.000","index":3},{"0":"Fossil Fuels","1":"2023-03-01","2":"185806050.250","index":4},{"0":"Fossil Fuels","1":"2023-04-01","2":"163770399.410","index":5},{"0":"All Other Fuels","1":"2023-06-01","2":"66296171.490","index":6},{"0":"Unknown","1":"2023-02-01","2":"0.000","index":7},{"0":"All Other Fuels","1":"2023-05-01","2":"62806718.530","index":8},{"0":"All Other Fuels","1":"2023-03-01","2":"64081044.870","index":9},{"0":"Unknown","1":"2023-08-01","2":"0.000","index":10},{"0":"Unknown","1":"2023-06-01","2":"0.000","index":11},{"0":"Renewable Fuels","1":"2023-03-01","2":"81306140.110","index":12},{"0":"Renewable Fuels","1":"2023-07-01","2":"72308202.330","index":13},{"0":"Renewable Fuels","1":"2023-08-01","2":"71901254.550","index":14},{"0":"Unknown","1":"2023-04-01","2":"0.000","index":15},{"0":"Fossil Fuels","1":"2023-06-01","2":"223180548.640","index":16},{"0":"Unknown","1":"2023-03-01","2":"0.000","index":17},{"0":"Renewable Fuels","1":"2023-01-01","2":"74377626.960","index":18},{"0":"Renewable Fuels","1":"2023-05-01","2":"81016963.220","index":19},{"0":"Renewable Fuels","1":"2023-02-01","2":"74331464.980","index":20},{"0":"Fossil Fuels","1":"2023-01-01","2":"202511960.840","index":21},{"0":"All Other Fuels","1":"2023-02-01","2":"62028959.610","index":22},{"0":"Unknown","1":"2023-01-01","2":"0.000","index":23},{"0":"All Other Fuels","1":"2023-01-01","2":"72244265.880","index":24},{"0":"Fossil Fuels","1":"2023-08-01","2":"282643683.010","index":25},{"0":"Fossil Fuels","1":"2023-05-01","2":"185311022.090","index":26},{"0":"Renewable Fuels","1":"2023-04-01","2":"79421669.100","index":27},{"0":"Unknown","1":"2023-05-01","2":"0.000","index":28},{"0":"All Other Fuels","1":"2023-04-01","2":"57811652.700","index":29},{"0":"Renewable Fuels","1":"2023-06-01","2":"68829493.060","index":30},{"0":"All Other Fuels","1":"2023-08-01","2":"71041584.420","index":31},{"0":"Fossil Fuels","1":"2023-02-01","2":"173950992.140","index":32}],"schema":[{"key":"0","name":"Fuel_class","type":"string"},{"key":"1","name":"Operating_date","type":"date"},{"key":"2","name":"Electricity_generated","type":"decimal"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"f6e89c77-99d8-469f-bc76-a6e6a5a259bb":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[],"schema":[{"key":"0","name":"Electricity generated","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"85089136-dbf6-4b2f-8e5a-2bc5a9e3242f":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[],"schema":[{"key":"0","name":"Electricity generated","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"34ae4b2e-5a41-494d-8b16-55c8a839438d"},{"id":"eb4c5c09-12bb-462c-b805-af2ec3684bc5"}],"default_lakehouse":"eb4c5c09-12bb-462c-b805-af2ec3684bc5","default_lakehouse_name":"EIA_Lake","default_lakehouse_workspace_id":"b4712af8-6388-4bac-85f2-139a486b8b92"}}},"nbformat":4,"nbformat_minor":5}