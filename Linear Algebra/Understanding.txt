What about vectors and matrices? A matrix is like a dataframe, so why are matrices and linear algebra so important?



Here's my response to your question about linear algebra. Linear algebra is important because that's the foundation of the math that is used to calculate a Y value given a set of X value inputs and their corresponding coefficients.  Similarly, neural networks use the same formula for producing a prediction (y = ax1 + bx2 + cx3 + etc).  As you can see, the output of the neural network is the sum of all the input features that have been multiplied by some coefficient. This "idea" or formula comes from linear algebra. It's just called linear because it comes from lines (think of the most simple line formula, y=mx+b)  Neural networks just do the same thing as this simple line formula, but add multiple terms, one for each input feature).  The coefficients of the terms are what is being optimized when the neural network is trained, typically by starting with random values, and then by using stochastic gradient descent, slowly adjusting the coefficients in such a way that decreases the overall loss of the formula.  So as you can see, there is a close relationship between neural networks and linear algebra because of the math used to produce an actual value from the model is the same as the basis for linear algebra.   Maybe that explains it, maybe it doesn't explain it enough. Let me know. 



Here's my response to your question about why matrices are important.  The short answer is because if we didn't use matrix math, it would take FOREVER to train a neural network.  GPU's are specialized for handling matrix math operations, and we can offload the processing to the GPU to have the calculations done WAY faster than by using our CPU.  So that's the short answer. The longer more involved answer is this. You can definitely train a neural network without using matrices at all.  The formulas can be calculated by hand, but's very arduous.  For example, during back propagation we need to calculate the gradient of each coefficient so we know how much it needs to be adjusted by in order to "learn".  Without using matrices, you would use the back propagation formula multiple times, which involves a formula using partial derrivirates, to calculate each coefficient one at a time.  This involves solving multiple math equations, one for each input feature.  For a network that might have 1,000,000 parameters (think image detection of a 1000x1000 pixel image, black and white) you would have to solve 1,000,000 math formulas.  Although computers CAN do that real fast, it's still too slow without using matrix math, because that's just one iteration. You might have to do thousands of iterations before your model loss reaches a global minimum.  With matrix math, however, you plug in those 1,000,000 parameters into one matrix and solve it with one matrix formula.  The GPU, thankfully, can do that very quickly because it has been optimized for matrix math.  Speaking of matrix math, many of the operations, like dot products work out the same as what you would use for linear algebra (y = ax1 + bx2 + cx3).  You are just taking the sum of the coefficients multiplied by the X values.  Matrix dot product does the EXACT SAME THING.  So that y = ax1 + bx2 + etc.. formula can be offloaded to the GPU.  Hopefully this helps you see why matrices are important. It's not just a data frame; it is a math construct that is conducive for solving the needed formulas for machine learning.  


Vectors are easy. Think of them as just being 1 dimensional matrices (or one "column" in a data frame, lol). Let me know if you have any specific questions about those. 
